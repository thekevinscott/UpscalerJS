---
title: MAXIM Enhancement
description: Overview of @upscalerjs/maxim-enhancement model
sidebar_position: 10
sidebar_label: maxim-enhancement
---

# MAXIM Enhancement

[![](https://data.jsdelivr.com/v1/package/npm/@upscalerjs/maxim-enhancement/badge)](https://www.jsdelivr.com/package/npm/@upscalerjs/maxim-enhancement)

MAXIM Enhancement is a collection of models for enhancing low light images.

The models were converted from weights provided by the [original MAXIM paper and repository](https://github.com/google-research/maxim). More information on the conversion process can be [found in this repository](https://github.com/upscalerjs/maxim).

## Samples + Demo

Here are some examples of processed images using these models.

import SampleTable from '@site/src/components/sampleTable/sampleTable';

<SampleTable
  packageName="maxim-enhancement"
  models={[
    'small',
    'medium',
    'large',
  ]}
  scales={[
    1,
    1,
    1,
  ]}
/>

import ModelExample from '@site/src/components/modelExample/modelExample';

<ModelExample model="maxim-enhancement" />


## Installation

```
npm install @upscalerjs/maxim-enhancement
```

## Usage

Import a model, specified by its weight:

```
import Upscaler from 'upscaler';
import small from '@upscalerjs/maxim-enhancement/small';

const upscaler = new Upscaler({
  model: small,
})
```

## Available Models

MAXIM Enhancement ships with three models of differing fidelity.

- small: `@upscalerjs/maxim-enhancement/small` - quantized `uint8`, input size of 64
- medium: `@upscalerjs/maxim-enhancement/medium` - quantized `uint8`, input size of 256
- large: `@upscalerjs/maxim-enhancement/large` - unquantized, input size of 256

All models are also exported via the root export:

```
import Upscaler from 'upscaler';
import models from '@upscalerjs/maxim-enhancement';

const upscaler = new Upscaler({
  model: models.small,
  // model: models.medium,
  // model: models.large,
})
```

### `small`

This model is quantized to `uint8` and has a fixed input size of 64. Because of the smaller input size, it will release the UI thread more often resulting in a more performant UI, but the overall speed of processing will be higher.

### `medium`

This model is quantized to `uint8` and has a fixed input size of 256. Because of the larger input size, this model will lock the UI thread for longer periods at a time, but the overall speed of processing will be lower.

### `large`

This model is unquantized and has a fixed input size of 256. This model is most appropriate for a GPU-accelerated Node environment, and will struggle to run on most browser hardware.


## Dataset
All weights were trained on the [LOL](https://paperswithcode.com/dataset/lol) dataset.


## License

[MIT License](https://oss.ninja/mit/developit/) Â© [Kevin Scott](https://thekevinscott.com)

